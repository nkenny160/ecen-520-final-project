{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":933.094964,"end_time":"2024-12-05T03:20:58.088883","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-05T03:05:24.993919","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ECEN 520 Final Project - XGboost + LSTM implementation\n## Download the data to begin processing steps","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\n\ndf = pd.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")\ndf_pl = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-12-06T05:17:45.730909Z","iopub.execute_input":"2024-12-06T05:17:45.731258Z","iopub.status.idle":"2024-12-06T05:17:51.594564Z","shell.execute_reply.started":"2024-12-06T05:17:45.731224Z","shell.execute_reply":"2024-12-06T05:17:51.593512Z"},"papermill":{"duration":0.446367,"end_time":"2024-12-05T03:05:48.549417","exception":false,"start_time":"2024-12-05T03:05:48.103050","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## There are a lot of partially or fully empty columns in each parquet so we need to identify these as the first step of preprocessing","metadata":{}},{"cell_type":"code","source":"# Three Cases\n# 1. Fully Empty columns : we can just drop them\n# 2. Partially Empty Columns: we can impute them\n# 3. Full Columns : No imputation necessary\n\nempty_columns = []\nfully_filled_columns = []\npartially_empty_columns = []\n\nfor feature in df_pl.columns:\n    # Count empty and non-empty rows\n    empty_rows = df_pl[feature].is_null().sum()\n    nonempty_rows = len(df_pl[feature]) - empty_rows\n\n    # Classify the columns based on the counts\n    if nonempty_rows == 0:\n        empty_columns.append(feature)\n    elif empty_rows == 0:\n        fully_filled_columns.append(feature)\n    else:\n        partially_empty_columns.append(feature)\n\n# We don't use df_pl again, so delete it to conserve memory\ndel df_pl","metadata":{"execution":{"iopub.status.busy":"2024-12-06T05:17:51.596228Z","iopub.execute_input":"2024-12-06T05:17:51.596966Z","iopub.status.idle":"2024-12-06T05:17:51.639186Z","shell.execute_reply.started":"2024-12-06T05:17:51.596911Z","shell.execute_reply":"2024-12-06T05:17:51.638346Z"},"papermill":{"duration":0.446367,"end_time":"2024-12-05T03:05:48.549417","exception":false,"start_time":"2024-12-05T03:05:48.103050","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next we drop the empty columns, sort the values by time, fill columns which are partially empty, and organize the columns into different groupings.","metadata":{}},{"cell_type":"code","source":"# drop empty columns using imputer\ndf = df.drop(empty_columns, axis = 1)\n\n# Fill in partially empty columns\nfor feature in partially_empty_columns:\n    df[feature] = df.groupby('symbol_id')[feature].transform(lambda x: x.ffill().bfill())\n\n# Split data temporally - in partition0 there are 170 days and 849 unique time ids per day\ndf = df.sort_values(['date_id', 'time_id'])\ndate_counts = df.date_id.value_counts()\ndate_counts = pd.DataFrame(date_counts.sort_index())\ndate_counts['cumulative_sum'] = date_counts['count'].cumsum()\n\n# Organize columns into different groups\nTIME_FEATURES = ['relative_timestamp']\nMARKET_FEATURES = [f'feature_{i:02}' for i in range(0,79) if f'feature_{i:02}' in df.columns]\nRESPONDER_FEATURES = [f'responder_{i}' for i in range(0,9) if f'responder_{i}' in df.columns]\nSYMBOL_FEATURES = ['symbol_id']","metadata":{"execution":{"iopub.status.busy":"2024-12-06T05:17:51.640512Z","iopub.execute_input":"2024-12-06T05:17:51.640774Z","iopub.status.idle":"2024-12-06T05:18:01.754326Z","shell.execute_reply.started":"2024-12-06T05:17:51.640745Z","shell.execute_reply":"2024-12-06T05:18:01.753448Z"},"papermill":{"duration":0.446367,"end_time":"2024-12-05T03:05:48.549417","exception":false,"start_time":"2024-12-05T03:05:48.103050","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Normalize all the responders and feature using standard scaler. \n\nThis is a little dicey because we don't know whether or not the features are already normalized, but this is a risk we are willing to take.","metadata":{}},{"cell_type":"code","source":"# Normalize Data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncols_to_normalize = MARKET_FEATURES + RESPONDER_FEATURES\ndf[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])","metadata":{"execution":{"iopub.status.busy":"2024-12-06T05:18:01.756311Z","iopub.execute_input":"2024-12-06T05:18:01.756653Z","iopub.status.idle":"2024-12-06T05:18:05.153487Z","shell.execute_reply.started":"2024-12-06T05:18:01.756624Z","shell.execute_reply":"2024-12-06T05:18:05.152470Z"},"papermill":{"duration":0.446367,"end_time":"2024-12-05T03:05:48.549417","exception":false,"start_time":"2024-12-05T03:05:48.103050","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split the data into train, validation, and testing sets. Also encode symbol_id so that it can be used as categorial data.\nWe are using the time index instead of train-test-split. We need to do this because train-test-split will choose random values which will cause our model to improperly train and overfit. This is because it is time-series data, so if we pick testing points in between the training points, these values will be much easier to predict since the model has seen the surrounding values. Our train, val, and test percentages are 60%, 20%, and 20% respectively. Additionally, before splitting into the sets and we encode the symbol_id using one-hot-encoder so that they are processed as categorical features instead of numerical features.","metadata":{}},{"cell_type":"code","source":"# Determine splitting points\ntotal = len(df)\ntrain_percentage = 0.6\nval_percentage = 0.2\ntest_percentage = 0.2\napprx_train_len = int(total*train_percentage)\napprx_val_len = int(total*val_percentage) \napprx_test_len = total - apprx_train_len - apprx_val_len\n\n# Determine Splitting points\ndef split_func(row):\n    s = row['cumulative_sum']\n    if s <= apprx_train_len:\n        return 'Train'\n    elif (s > apprx_train_len) and (s <= apprx_train_len + apprx_val_len):\n        return 'Val'\n    elif (s > apprx_train_len + apprx_val_len):\n        return 'Test'\n    else:\n        raise ValueError\n\ndate_counts['Split'] = date_counts.apply(split_func, axis = 1)\nlast_train_data = date_counts[date_counts.Split == 'Train'].tail(1) \nfirst_test_data = date_counts[date_counts.Split == 'Test'].head(1)\n\n# Once Splitting points are determined, then make the necessary splits\ndef split_func_df(row):\n    s = row['date_id']\n    if s <= last_train_data.index[0]:\n        return 'Train'\n    elif (s > last_train_data.index[0]) and (s < first_test_data.index[0]):\n        return 'Val'\n    elif (s >= first_test_data.index[0]):\n        return 'Test'\n    else:\n        raise ValueError\n\n# Encode the symbol_id using one-hot-encoder so that they are processed as categorical features instead of numerical features\ndf = df[df.symbol_id.isin(range(10))]\nencoded = pd.get_dummies(df['symbol_id'], prefix='symbol_id')\nmax_symbol_id = df['symbol_id'].max()\nencoded.head()\ndf = pd.concat([df, encoded], axis = 1)\ndf = df.drop(['symbol_id'], axis = 1)\n\ndf['Split'] = df.apply(split_func_df, axis = 1)\ndf['Split'].value_counts()\n\ntrain_df = df[df.Split == 'Train']\nval_df = df[df.Split == 'Val']\ntest_df = df[df.Split == 'Test']","metadata":{"execution":{"iopub.status.busy":"2024-12-06T05:18:05.154802Z","iopub.execute_input":"2024-12-06T05:18:05.155383Z","iopub.status.idle":"2024-12-06T05:18:18.229213Z","shell.execute_reply.started":"2024-12-06T05:18:05.155337Z","shell.execute_reply":"2024-12-06T05:18:18.228447Z"},"papermill":{"duration":0.446367,"end_time":"2024-12-05T03:05:48.549417","exception":false,"start_time":"2024-12-05T03:05:48.103050","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Now that preprocessing is done, we can start implementing our models. First we apply XGboost.\nXGboost is used to create additional features for predicting the value of responder_6. We do this because XGboost is not strictly a time-series model so it is able to capture different aspects of the dataset than the LSTM model. XGboost is able to capture non-temporal relationships and provide a prediction for responder_6 that can help the LSTM model come to a better conclusion of the actual value of responder_6. We use squarederror as the XGboost objective since we are trying to predict a numerical value. The number of gradient-boosting trees is set by n_estimators, and we use a value of 100. This number is the default value, and we achieved good results so we did not tune it further. Better tuning of the XGboost model is definitely possible.","metadata":{}},{"cell_type":"code","source":"# import xgboost as xgb\n\n# # General Feature set (used for training LSTM)\nfeatures = ['date_id', 'time_id', 'feature_06', 'feature_07', 'feature_05', 'feature_68', 'responder_6']+list(encoded.columns)\n# # Define features for XGBoost (does not include responder_6 so that we can train XGboost)\n# xgb_features = ['date_id', 'time_id', 'feature_06', 'feature_07', 'feature_05', 'feature_68'] + list(encoded.columns)\n\n# # Train XGBoost\n# xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n# xgb_model.fit(train_df[xgb_features], train_df['responder_6'])\n\n# # Make predictions\n# train_df['xgb_pred'] = xgb_model.predict(train_df[xgb_features])\n# val_df['xgb_pred'] = xgb_model.predict(val_df[xgb_features])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T05:18:18.230305Z","iopub.execute_input":"2024-12-06T05:18:18.230679Z","iopub.status.idle":"2024-12-06T05:18:21.191961Z","shell.execute_reply.started":"2024-12-06T05:18:18.230635Z","shell.execute_reply":"2024-12-06T05:18:21.191195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Why don't you actually use Xgboost after hyping it up?\nWhile we show an implementation for XGboost here, having this cascade into the LSTM model honestly did not improve the results and led to increased overfitting compared to just a standard LSTM model. We think with better tuning it is possible to have an Xgboost model feed into the LSTM model, but we currently do not have the time to tune this to perfection.","metadata":{}},{"cell_type":"markdown","source":"## Now that we have all the features that we would like to use, we can train the Long Short Term Memory (LSTM) neural network model. \n\nFor training, we are using the top-4 features and the additional prediction from the XGboost model. We use the top-4 features because this makes it possible to train the model given the limited Kaggle resources. These features were chosen using a correlation matrix which is detailed in the VAR notebook. A future implementation could use PCA instead of just choosing these K-features. Note, that unlike a VAR model,the time-series data is left as distinct dates and times instead of combining them into one index.","metadata":{}},{"cell_type":"markdown","source":"## The first step for implementing a neural network is to split the training and validation data into sequences of a fixed length. \nFor this training, we are using an arbitrary sequence length of 50, but there is potential that this could be tweaked for better results.","metadata":{}},{"cell_type":"code","source":"# Used to completely reset variables for more thorough model experimentation without having to repeat previous steps\n# del X_train, y_train, X_val, y_val, train_data, val_data, X_test, y_test, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:21:56.439646Z","iopub.execute_input":"2024-12-06T06:21:56.440065Z","iopub.status.idle":"2024-12-06T06:22:00.676930Z","shell.execute_reply.started":"2024-12-06T06:21:56.440030Z","shell.execute_reply":"2024-12-06T06:22:00.675863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n# features_with_xgb = features + ['xgb_pred']\n\ndef create_sequences(data, sequence_length):\n    X, y = [], []\n    for i in range(len(data) - sequence_length):\n        seq = data[i:i+sequence_length]\n        target = data['responder_6'].iloc[i+sequence_length]\n        X.append(seq)\n        y.append(target)\n    return np.array(X), np.array(y)\n\n# Create sequences\n# X_train, y_train = create_sequences(train_df[features_with_xgb], sequence_length=50)\n# X_val, y_val = create_sequences(val_df[features_with_xgb], sequence_length=50)\nX_train, y_train = create_sequences(train_df[features], sequence_length=50)\nX_val, y_val = create_sequences(val_df[features], sequence_length=50)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T06:22:15.844664Z","iopub.execute_input":"2024-12-06T06:22:15.845411Z","iopub.status.idle":"2024-12-06T06:23:33.600674Z","shell.execute_reply.started":"2024-12-06T06:22:15.845371Z","shell.execute_reply":"2024-12-06T06:23:33.599913Z"},"papermill":{"duration":75.924846,"end_time":"2024-12-05T03:07:17.120466","exception":false,"start_time":"2024-12-05T03:06:01.195620","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The next step is to define the model.\nFor this case, we found that implementing two LSTM layers and one dense layer (necessary for regression) produced good results, but other combinations are definitely possible. Using \"tanh\" as the activation function yielded fair results, and in our testing, ReLu actually performed worse overall both in terms of accuracy and also the drastic increase in train time per epoch. We used Additionally, we also added optimizations that allow for the tensor processing cores on a TPU or specific GPUs (like the Nvidia T4) can be used to speed up the training process. We started out training with 5 epochs, and add further training later.","metadata":{}},{"cell_type":"code","source":"# Used for testing various iterations of the model (clear memory of prior model) without needing to rerun all the prior steps\n# del model, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:44:14.327642Z","iopub.execute_input":"2024-12-06T06:44:14.328365Z","iopub.status.idle":"2024-12-06T06:44:14.332391Z","shell.execute_reply.started":"2024-12-06T06:44:14.328327Z","shell.execute_reply":"2024-12-06T06:44:14.331400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import TensorBoard\n# Create TensorFlow datasets to optimize for running on Nvidia T4s\nbatch_size = 128\ntrain_data = tf.data.Dataset.from_tensor_slices((X_train.astype('float'), y_train.astype('float'))).batch(batch_size).prefetch(tf.data.AUTOTUNE)\nval_data = tf.data.Dataset.from_tensor_slices((X_val.astype('float'), y_val.astype('float'))).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# TensorBoard callback with profiling\ntensorboard_cb = TensorBoard(log_dir=\"./logs\", profile_batch='2,10')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:27:01.937212Z","iopub.execute_input":"2024-12-06T06:27:01.937619Z","iopub.status.idle":"2024-12-06T06:27:27.957633Z","shell.execute_reply.started":"2024-12-06T06:27:01.937583Z","shell.execute_reply":"2024-12-06T06:27:27.955571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bidirectional vs Unidirectional LSTM Neural Networks\nThere are two models implemented in the following code, bidirectional and unidirectional LSTM. The benefit of a bidirectional approach is that it allows the model to capture context from both the past and the future. As a result, bidirectional LSTM networks can achieve better performance on tasks where context from both directions is crucial. While future data is what we are trying to predict in this case, financial markets are somewhat cyclical in nature meaning that patterns can repeat themselves, so bidirectional modeling could be a good approach for this dataset.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.mixed_precision import set_global_policy\nfrom tensorflow.keras.layers import Bidirectional\n\n# Primary function of these two lines is to enable optimization using tensor cores\nset_global_policy('mixed_float16')\ntf.config.optimizer.set_jit(True)\n\n# Define the model\n# model = Sequential([\n#     LSTM(512, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n#     Dropout(0.2),\n#     LSTM(256, activation='tanh'),\n#     Dropout(0.2),\n#     Dense(1)  # Output layer for regression\n# ])\nmodel = Sequential([\n    Bidirectional(LSTM(128, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))),\n    Dropout(0.2),\n    Bidirectional(LSTM(64, activation='tanh', return_sequences=True)),\n    Dropout(0.2),\n    Bidirectional(LSTM(32, activation='tanh')),\n    Dropout(0.2),\n    Dense(1)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\nmodel.summary()\n\n# Train the model\nhistory = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=20,\n    callbacks=[tensorboard_cb]\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T06:44:42.942153Z","iopub.execute_input":"2024-12-06T06:44:42.942537Z","iopub.status.idle":"2024-12-06T06:50:11.521731Z","shell.execute_reply.started":"2024-12-06T06:44:42.942505Z","shell.execute_reply":"2024-12-06T06:50:11.520979Z"},"papermill":{"duration":75.924846,"end_time":"2024-12-05T03:07:17.120466","exception":false,"start_time":"2024-12-05T03:06:01.195620","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## The following sections are just training the model with more epochs to get the desired results.\nThis only runs if required.","metadata":{}},{"cell_type":"code","source":"# history = model.fit(\n#     train_data,\n#     validation_data=val_data,\n#     epochs=15,\n#     verbose=1,\n#     callbacks=[tensorboard_cb]\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:53:00.825062Z","iopub.execute_input":"2024-12-06T06:53:00.825502Z","iopub.status.idle":"2024-12-06T07:08:40.981106Z","shell.execute_reply.started":"2024-12-06T06:53:00.825466Z","shell.execute_reply":"2024-12-06T07:08:40.980285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## At this point the validation loss and training loss are not reducing further and are fairly close in value, so I think it is not worth continuing to train on more epochs. Let's save the model for future use.","metadata":{}},{"cell_type":"code","source":"model.save('model.h5')","metadata":{"execution":{"iopub.status.busy":"2024-12-05T17:37:11.699193Z","iopub.execute_input":"2024-12-05T17:37:11.699539Z","iopub.status.idle":"2024-12-05T17:37:11.730599Z","shell.execute_reply.started":"2024-12-05T17:37:11.699508Z","shell.execute_reply":"2024-12-05T17:37:11.729709Z"},"papermill":{"duration":0.62581,"end_time":"2024-12-05T03:20:16.933810","exception":false,"start_time":"2024-12-05T03:20:16.308000","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Next create the testing sequences so we can test the model on fresh data.","metadata":{}},{"cell_type":"code","source":"X_test, y_test = create_sequences(test_df[features], sequence_length=50)\ntest_data = tf.data.Dataset.from_tensor_slices((X_test.astype('float'), y_test.astype('float'))).batch(batch_size).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T06:36:06.369596Z","iopub.execute_input":"2024-12-06T06:36:06.370479Z","iopub.status.idle":"2024-12-06T06:36:32.181173Z","shell.execute_reply.started":"2024-12-06T06:36:06.370442Z","shell.execute_reply":"2024-12-06T06:36:32.180131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = model.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T07:30:46.480207Z","iopub.execute_input":"2024-12-06T07:30:46.480562Z","iopub.status.idle":"2024-12-06T07:30:54.355151Z","shell.execute_reply.started":"2024-12-06T07:30:46.480531Z","shell.execute_reply":"2024-12-06T07:30:54.354271Z"},"papermill":{"duration":30.182484,"end_time":"2024-12-05T03:20:47.731768","exception":false,"start_time":"2024-12-05T03:20:17.549284","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This loss is much higher than the training set indicating that the model is overfitting by a fair margin and more tuning is required.","metadata":{}},{"cell_type":"code","source":"s_0 = test_df[test_df.symbol_id_0 == 1]\n\nlen(s_0)\n\nsample_df = s_0.iloc[::100]\n\nprint(len(sample_df))\n\nX_s, y_s = create_sequences(sample_df[features], sequence_length=50)\n\nsample_data = tf.data.Dataset.from_tensor_slices((X_s.astype('float'))).batch(batch_size).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-12-06T07:20:51.422487Z","iopub.execute_input":"2024-12-06T07:20:51.422874Z","iopub.status.idle":"2024-12-06T07:20:51.500120Z","shell.execute_reply.started":"2024-12-06T07:20:51.422840Z","shell.execute_reply":"2024-12-06T07:20:51.499126Z"},"papermill":{"duration":0.6539,"end_time":"2024-12-05T03:20:48.998482","exception":false,"start_time":"2024-12-05T03:20:48.344582","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Finally plot the results of the running the model with the test set.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Get the data for responder_6 in the test set\n\nresponder_6_predictions = model.predict(sample_data)\n\nresponder_6_actual_values = y_s\n\n# Plot the actual vs predicted values\n\nplt.figure(figsize=(10, 6))  # Adjust figure size as needed \n\nplt.plot(range(len(responder_6_actual_values)), responder_6_actual_values, label='Actual Values')\n\nplt.plot(range(len(responder_6_predictions.squeeze())), responder_6_predictions.squeeze(), label='Predicted Values')  # Squeeze to remove extra dimension\n\nplt.xlabel('Time Step')\n\nplt.ylabel('Values')\n\nplt.title(f'Responder 6 - Actual vs Predicted Values')\n\nplt.legend()\n\nplt.grid(True)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-06T07:20:53.933240Z","iopub.execute_input":"2024-12-06T07:20:53.933597Z","iopub.status.idle":"2024-12-06T07:20:55.945377Z","shell.execute_reply.started":"2024-12-06T07:20:53.933564Z","shell.execute_reply":"2024-12-06T07:20:55.944434Z"},"papermill":{"duration":1.23578,"end_time":"2024-12-05T03:20:52.095509","exception":false,"start_time":"2024-12-05T03:20:50.859729","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusions\nInterestingly enough, the difference between the MSE of the train/val and test sets for this model is much smaller than that of all the other models that we tested. However, I think the difference between the results we still see can be explained by the fact that the test set is completely new data to the model that has little resemblance to the data that it was trained on. To improve this relationship, one could potentially implement better tuning on both the XGboost and LSTM model to reduce overfitting and increase the correlation between the features and responders. Overall, I think these results are acceptable for the time being (and given our skill level).","metadata":{}}]}