{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\ndf = pd.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")\ndf_pl = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:06:09.526447Z","iopub.execute_input":"2024-11-27T03:06:09.527485Z","iopub.status.idle":"2024-11-27T03:06:16.108837Z","shell.execute_reply.started":"2024-11-27T03:06:09.527435Z","shell.execute_reply":"2024-11-27T03:06:16.107850Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Three Cases\n# 1. Fully Empty columns : we can just drop them\n# 2. Partially Empty Columns: we can impute them\n# 3. Full Columns : No imputation necessary\n\nempty_columns = []\nfully_filled_columns = []\npartially_empty_columns = []\n\nfor feature in df_pl.columns:\n    # Count empty and non-empty rows\n    empty_rows = df_pl[feature].is_null().sum()\n    nonempty_rows = len(df_pl[feature]) - empty_rows\n\n    # Classify the columns based on the counts\n    if nonempty_rows == 0:\n        empty_columns.append(feature)\n    elif empty_rows == 0:\n        fully_filled_columns.append(feature)\n    else:\n        partially_empty_columns.append(feature)\n\n# We don't use df_pl again, so delete it to conserve memory\ndel df_pl\n\ndf.sort_values(['time_id','date_id'])\n\n# drop empty columns using imputer\ndf = df.drop(empty_columns, axis = 1)\n\n# Fill in partially empty columns\nfor feature in partially_empty_columns:\n    df[feature] = df.groupby('symbol_id')[feature].transform(lambda x: x.ffill().bfill())\n\n# Split data temporally - in partition0 there are 170 days and 849 unique time ids per day\ndf = df.sort_values(['date_id', 'time_id'])\ndate_counts = df.date_id.value_counts()\ndate_counts = pd.DataFrame(date_counts.sort_index())\ndate_counts['cumulative_sum'] = date_counts['count'].cumsum()\n\ntotal = len(df)\ntrain_percentage = 0.6\nval_percentage = 0.2\ntest_percentage = 0.2\napprx_train_len = int(total*train_percentage)\napprx_val_len = int(total*val_percentage) \napprx_test_len = total - apprx_train_len - apprx_val_len\n\n# Determine Splitting points\n\ndef split_func(row):\n    s = row['cumulative_sum']\n    if s <= apprx_train_len:\n        return 'Train'\n    elif (s > apprx_train_len) and (s <= apprx_train_len + apprx_val_len):\n        return 'Val'\n    elif (s > apprx_train_len + apprx_val_len):\n        return 'Test'\n    else:\n        raise ValueError\n\ndate_counts['Split'] = date_counts.apply(split_func, axis = 1)\n# print(date_counts.Split.value_counts())\n\nlast_train_data = date_counts[date_counts.Split == 'Train'].tail(1) \nfirst_test_data = date_counts[date_counts.Split == 'Test'].head(1)\n\n# Once Splitting points are determined, then make the necessary splits\n \ndef split_func_df(row):\n    s = row['date_id']\n    if s <= last_train_data.index[0]:\n        return 'Train'\n    elif (s > last_train_data.index[0]) and (s < first_test_data.index[0]):\n        return 'Val'\n    elif (s >= first_test_data.index[0]):\n        return 'Test'\n    else:\n        raise ValueError\n\n\ndf['Split'] = df.apply(split_func_df, axis = 1)\ndf['Split'].value_counts()\n\ntrain_df = df[df.Split == 'Train']\nval_df = df[df.Split == 'Val']\ntest_df = df[df.Split == 'Test']\n\n# Import libraries\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n\n# Setup steps to sort columns into different categories\n\nTEMPORAL_FEATURES = ['date_id', 'time_id','symbol_id']\nMARKET_FEATURES = [f'feature_{i:02}' for i in range(0,79) if f'feature_{i:02}' in df.columns]\nRESPONDER_FEATURES = [f'responder_{i}' for i in range(0,9) if f'responder_{i}' in df.columns]\nRESPONDER_FEATURES.remove('responder_6')\nSYMBOL_FEATURES = ['symbol_id']\n# SYMBOL_FEATURES = [f'symbol_id_{i}' for i in range(max_symbol_id) if f'symbol_id_{i}' in df.columns]\nWEIGHT = ['WEIGHT']\n\n# Assemble Features\nALL_FEATURES = MARKET_FEATURES + RESPONDER_FEATURES\nALL_FEATURES = ALL_FEATURES + SYMBOL_FEATURES\n\n# train_x = train_df[ALL_FEATURES]\n# train_y = train_df[['responder_6']]\n\n# val_x = val_df[ALL_FEATURES]\n# val_y = val_df[['responder_6']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:06:20.448158Z","iopub.execute_input":"2024-11-27T03:06:20.448704Z","iopub.status.idle":"2024-11-27T03:06:50.164613Z","shell.execute_reply.started":"2024-11-27T03:06:20.448656Z","shell.execute_reply":"2024-11-27T03:06:50.163539Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"correlation_results = []\ntrain_x_no_responder = train_df[MARKET_FEATURES]\nfor feature in train_x_no_responder.columns:\n    correlation = train_x_no_responder[feature].corr(train_y['responder_6'])\n    correlation_spear = train_x_no_responder[feature].corr(train_y['responder_6'], method='spearman')\n    correlation_results.append((feature, abs(correlation), abs(correlation_spear)))\n\n# Create a DataFrame from the results\ncorrelation_df = pd.DataFrame(correlation_results, columns=['Feature', 'Pearson Correlation', 'Spearman Correlation'])\n\n# Sort by the absolute value of Pearson correlation (you can choose Spearman if needed)\ncorrelation_df = correlation_df.sort_values(by='Pearson Correlation', ascending=False)\n\n# Print the sorted results\nprint(correlation_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T02:27:41.803877Z","iopub.execute_input":"2024-11-27T02:27:41.804309Z","iopub.status.idle":"2024-11-27T02:28:18.520614Z","shell.execute_reply.started":"2024-11-27T02:27:41.804275Z","shell.execute_reply":"2024-11-27T02:28:18.519360Z"}},"outputs":[{"name":"stdout","text":"       Feature  Pearson Correlation  Spearman Correlation\n1   feature_06             0.087920              0.097684\n2   feature_07             0.069612              0.083678\n0   feature_05             0.043499              0.034776\n59  feature_68             0.029816              0.018951\n3   feature_08             0.028897              0.015678\n..         ...                  ...                   ...\n17  feature_23             0.000914              0.001027\n16  feature_22             0.000688              0.000892\n52  feature_61             0.000533              0.002069\n46  feature_55             0.000484              0.012719\n15  feature_20             0.000352              0.002418\n\n[70 rows x 3 columns]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# VAR model testing","metadata":{}},{"cell_type":"markdown","source":"Make a smaller dataset for testing the VAR model (very memory intensive)","metadata":{}},{"cell_type":"code","source":"train_x = train_df[['date_id', 'time_id','symbol_id','feature_06', 'feature_07', 'feature_05', 'feature_68']]\ntrain_y = train_df[['responder_6']]\n\nval_x = val_df[['date_id', 'time_id','symbol_id','feature_06', 'feature_07', 'feature_05', 'feature_68']]\nval_y = val_df[['responder_6']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:07:03.930197Z","iopub.execute_input":"2024-11-27T03:07:03.930771Z","iopub.status.idle":"2024-11-27T03:07:03.950611Z","shell.execute_reply.started":"2024-11-27T03:07:03.930731Z","shell.execute_reply":"2024-11-27T03:07:03.949258Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\n\nmodel = VAR(train_x)  # train_data should be a pandas DataFrame\nlags = model.select_order()\nprint(lags.summary())\n# results = model.fit(lags)  # Determine the optimal lag using `model.select_order()`\n# forecast = results.forecast(train_data.values[-lags:], steps=steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T03:07:17.367057Z","iopub.execute_input":"2024-11-27T03:07:17.367489Z"}},"outputs":[],"execution_count":null}]}