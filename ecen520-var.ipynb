{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\ndf = pd.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")\ndf_pl = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T01:52:52.143094Z","iopub.execute_input":"2024-11-27T01:52:52.143629Z","iopub.status.idle":"2024-11-27T01:52:59.420479Z","shell.execute_reply.started":"2024-11-27T01:52:52.143572Z","shell.execute_reply":"2024-11-27T01:52:59.419122Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Three Cases\n# 1. Fully Empty columns : we can just drop them\n# 2. Partially Empty Columns: we can impute them\n# 3. Full Columns : No imputation necessary\n\nempty_columns = []\nfully_filled_columns = []\npartially_empty_columns = []\n\nfor feature in df_pl.columns:\n    # Count empty and non-empty rows\n    empty_rows = df_pl[feature].is_null().sum()\n    nonempty_rows = len(df_pl[feature]) - empty_rows\n\n    # Classify the columns based on the counts\n    if nonempty_rows == 0:\n        empty_columns.append(feature)\n    elif empty_rows == 0:\n        fully_filled_columns.append(feature)\n    else:\n        partially_empty_columns.append(feature)\n\n# We don't use df_pl again, so delete it to conserve memory\ndel df_pl\n\ndf.sort_values(['time_id','date_id'])\n\n# drop empty columns using imputer\ndf = df.drop(empty_columns, axis = 1)\n\n# Fill in partially empty columns\nfor feature in partially_empty_columns:\n    df[feature] = df.groupby('symbol_id')[feature].transform(lambda x: x.ffill().bfill())\n\n# Split data temporally - in partition0 there are 170 days and 849 unique time ids per day\ndf = df.sort_values(['date_id', 'time_id'])\ndate_counts = df.date_id.value_counts()\ndate_counts = pd.DataFrame(date_counts.sort_index())\ndate_counts['cumulative_sum'] = date_counts['count'].cumsum()\n\ntotal = len(df)\ntrain_percentage = 0.6\nval_percentage = 0.2\ntest_percentage = 0.2\napprx_train_len = int(total*train_percentage)\napprx_val_len = int(total*val_percentage) \napprx_test_len = total - apprx_train_len - apprx_val_len\n\n# Determine Splitting points\n\ndef split_func(row):\n    s = row['cumulative_sum']\n    if s <= apprx_train_len:\n        return 'Train'\n    elif (s > apprx_train_len) and (s <= apprx_train_len + apprx_val_len):\n        return 'Val'\n    elif (s > apprx_train_len + apprx_val_len):\n        return 'Test'\n    else:\n        raise ValueError\n\ndate_counts['Split'] = date_counts.apply(split_func, axis = 1)\n# print(date_counts.Split.value_counts())\n\nlast_train_data = date_counts[date_counts.Split == 'Train'].tail(1) \nfirst_test_data = date_counts[date_counts.Split == 'Test'].head(1)\n\n# Once Splitting points are determined, then make the necessary splits\n \ndef split_func_df(row):\n    s = row['date_id']\n    if s <= last_train_data.index[0]:\n        return 'Train'\n    elif (s > last_train_data.index[0]) and (s < first_test_data.index[0]):\n        return 'Val'\n    elif (s >= first_test_data.index[0]):\n        return 'Test'\n    else:\n        raise ValueError\n\n\ndf['Split'] = df.apply(split_func_df, axis = 1)\ndf['Split'].value_counts()\n\ntrain_df = df[df.Split == 'Train']\nval_df = df[df.Split == 'Val']\ntest_df = df[df.Split == 'Test']\n\n# Import libraries\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n\n# Setup steps to sort columns into different categories\n\nTEMPORAL_FEATURES = ['date_id', 'time_id','symbol_id']\nMARKET_FEATURES = [f'feature_{i:02}' for i in range(0,79) if f'feature_{i:02}' in df.columns]\nRESPONDER_FEATURES = [f'responder_{i}' for i in range(0,9) if f'responder_{i}' in df.columns]\nRESPONDER_FEATURES.remove('responder_6')\nSYMBOL_FEATURES = ['symbol_id']\n# SYMBOL_FEATURES = [f'symbol_id_{i}' for i in range(max_symbol_id) if f'symbol_id_{i}' in df.columns]\nWEIGHT = ['WEIGHT']\n\n# Assemble Features\nALL_FEATURES = MARKET_FEATURES + RESPONDER_FEATURES\nALL_FEATURES = ALL_FEATURES + SYMBOL_FEATURES\n\ntrain_x = train_df[ALL_FEATURES]\ntrain_y = train_df[['responder_6']]\n\nval_x = val_df[ALL_FEATURES]\nval_y = val_df[['responder_6']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T01:53:02.251188Z","iopub.execute_input":"2024-11-27T01:53:02.251678Z","iopub.status.idle":"2024-11-27T01:53:32.680344Z","shell.execute_reply.started":"2024-11-27T01:53:02.251642Z","shell.execute_reply":"2024-11-27T01:53:32.679078Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# VAR model testing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\n\nmodel = VAR(train_x)  # train_data should be a pandas DataFrame\nlags = model.select_order()\nprint(lags.summary())\n# results = model.fit(lags)  # Determine the optimal lag using `model.select_order()`\n# forecast = results.forecast(train_data.values[-lags:], steps=steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T01:53:55.329598Z","iopub.execute_input":"2024-11-27T01:53:55.330012Z"}},"outputs":[],"execution_count":null}]}