{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the data"
      ],
      "metadata": {
        "id": "clqZ5_UVcMs-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwtWbpLzZaRf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import polars as pl\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")\n",
        "df_pl = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the DataFrame\n",
        "df = df.sort_values(by=['symbol_id', 'date_id', 'time_id'])\n",
        "df = df.reset_index(drop=True)\n",
        "df.head(100)"
      ],
      "metadata": {
        "id": "N7wYm0-maTvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the data"
      ],
      "metadata": {
        "id": "AIpyjwAocQW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "y_Tk55zUa9TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "empty_columns = []\n",
        "fully_filled_columns = []\n",
        "partially_empty_columns = []\n",
        "\n",
        "## pl\n",
        "\n",
        "for feature in df_pl.columns:\n",
        "    # Count empty and non-empty rows\n",
        "    empty_rows = df_pl[feature].is_null().sum()\n",
        "    nonempty_rows = len(df_pl[feature]) - empty_rows\n",
        "\n",
        "    # Classify the columns based on the counts\n",
        "    if nonempty_rows == 0:\n",
        "        empty_columns.append(feature)\n",
        "    elif empty_rows == 0:\n",
        "        fully_filled_columns.append(feature)\n",
        "    else:\n",
        "        partially_empty_columns.append(feature)\n",
        "\n",
        "    # Print feature statistics\n",
        "    print(f'{feature} : total - {len(df_pl[feature])} - empty - {empty_rows} - nonempty - {nonempty_rows}')\n"
      ],
      "metadata": {
        "id": "s62tW-XUa9uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "    display(df.describe().drop(['date_id', 'time_id', 'symbol_id', 'weight'], axis = 1).T.style.background_gradient(cmap='coolwarm'))"
      ],
      "metadata": {
        "id": "CZfR3Z14cTp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.date_id==0].symbol_id.unique()\n",
        "df[df.date_id==1].symbol_id.unique()\n",
        "df[df.symbol_id == 1].date_id.unique()\n",
        "set(df[df.symbol_id == 1].date_id.unique()) - set(df[df.symbol_id == 7].date_id.unique())"
      ],
      "metadata": {
        "id": "dT8StHNlbSqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask1 = (df.symbol_id == 1) & (df.date_id == 0)\n",
        "mask2 = (df.symbol_id == 7) & (df.date_id == 0)\n",
        "list(set(df[mask1].time_id.unique()) - set(df[mask2].date_id.unique()))[:10]"
      ],
      "metadata": {
        "id": "uXVI3D-rbZIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing"
      ],
      "metadata": {
        "id": "9TJ3esSkcWIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One Hot Encoding"
      ],
      "metadata": {
        "id": "hcNKbrwodAnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = pd.get_dummies(df['symbol_id'], prefix='symbol_id')\n",
        "max_symbol_id = df['symbol_id'].max()\n",
        "print(max_symbol_id)\n",
        "encoded.head()"
      ],
      "metadata": {
        "id": "jA2tLPbBcbYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df, encoded], axis = 1)\n",
        "df = df.drop(['symbol_id'], axis = 1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dvrOALkJdDBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Percentage Based Column Filtering"
      ],
      "metadata": {
        "id": "7tHeU2bZcnmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_groups = pd.DataFrame(columns = ['Symbol ID', 'Dropped Columns', 'Filled Columns', '% Filled'])\n",
        "\n",
        "percentage_threshold = 10.0\n",
        "\n",
        "for symbol_id, group in df.groupby('symbol_id'):\n",
        "\n",
        "    # print(f\"\\nSymbol ID: {symbol_id}\")\n",
        "    # processed_groups['Symbol ID'].append(symbol_id)\n",
        "\n",
        "    dropped_columns = []\n",
        "    filled_columns = []\n",
        "    percent_filled = []\n",
        "\n",
        "    for feature in group.columns:\n",
        "        total_rows = len(group)\n",
        "        empty_rows = group[feature].isnull().sum()\n",
        "        empty_rows_percentage = (empty_rows / total_rows) * 100\n",
        "        nonempty_rows_percentage = 100 - empty_rows_percentage\n",
        "\n",
        "        # print(f'{feature} : empty row = {empty_rows_percentage:.2f}% - non empty rows = {nonempty_rows_percentage:.2f}%')\n",
        "\n",
        "        if empty_rows == total_rows or empty_rows_percentage >= percentage_threshold:\n",
        "            df = df.drop(columns=feature)\n",
        "            dropped_columns.append(feature)\n",
        "\n",
        "        elif empty_rows > 0:\n",
        "            # Forward-fill and backward-fill missing values\n",
        "            df.loc[group.index,feature] = group[feature].ffill().bfill()\n",
        "            filled_columns.append(feature)\n",
        "            percent_filled.append(empty_rows_percentage)\n",
        "\n",
        "    # print(f\"Symbol ID: {symbol_id}\")\n",
        "    # print(f\"Dropped Columns: {dropped_columns}\")\n",
        "    # print(f\"Filled Columns: {filled_columns}\")\n",
        "    # print(f\"% Filled: {percent_filled}\")\n",
        "\n",
        "\n",
        "\n",
        "    # Check if there is data to add to the DataFrame\n",
        "\n",
        "    if dropped_columns or filled_columns or percent_filled:\n",
        "        # new row for table\n",
        "        new_row = pd.DataFrame({\n",
        "            'Symbol ID': [symbol_id],\n",
        "            'Dropped Columns': [dropped_columns],\n",
        "            'Filled Columns': [filled_columns],\n",
        "            '% Filled': [percent_filled]\n",
        "        })\n",
        "\n",
        "\n",
        "    processed_groups = pd.concat([processed_groups, new_row], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "print(processed_groups)"
      ],
      "metadata": {
        "id": "Bi21unxKalN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Temporal Splitting"
      ],
      "metadata": {
        "id": "1eXi6B0-dJkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sort_values(['date_id', 'time_id'])\n",
        "date_counts = df.date_id.value_counts()"
      ],
      "metadata": {
        "id": "Jz4NKoAOdL0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_counts = pd.DataFrame(date_counts.sort_index())\n",
        "date_counts['cumulative_sum'] = date_counts['count'].cumsum()\n",
        "date_counts.head()"
      ],
      "metadata": {
        "id": "_YiNmZNRdNsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total = len(df)\n",
        "train_percentage = 0.6\n",
        "val_percentage = 0.2\n",
        "test_percentage = 0.2\n",
        "apprx_train_len = int(total*train_percentage)\n",
        "apprx_val_len = int(total*val_percentage)\n",
        "apprx_test_len = total - apprx_train_len - apprx_val_len\n",
        "\n",
        "def split_func(row):\n",
        "    s = row['cumulative_sum']\n",
        "    if s <= apprx_train_len:\n",
        "        return 'Train'\n",
        "    elif (s > apprx_train_len) and (s <= apprx_train_len + apprx_val_len):\n",
        "        return 'Val'\n",
        "    elif (s > apprx_train_len + apprx_val_len):\n",
        "        return 'Test'\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "date_counts['Split'] = date_counts.apply(split_func, axis = 1)\n",
        "print(date_counts.Split.value_counts())\n",
        "date_counts.head()"
      ],
      "metadata": {
        "id": "YijHbH56dPnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_train_data = date_counts[date_counts.Split == 'Train'].tail(1)\n",
        "first_test_data = date_counts[date_counts.Split == 'Test'].head(1)\n"
      ],
      "metadata": {
        "id": "HzZ8ZxiCdRiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def split_func_df(row):\n",
        "    s = row['date_id']\n",
        "    if s <= last_train_data.index[0]:\n",
        "        return 'Train'\n",
        "    elif (s > last_train_data.index[0]) and (s < first_test_data.index[0]):\n",
        "        return 'Val'\n",
        "    elif (s >= first_test_data.index[0]):\n",
        "        return 'Test'\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "\n",
        "df['Split'] = df.apply(split_func_df, axis = 1)\n",
        "df['Split'].value_counts()"
      ],
      "metadata": {
        "id": "6C8OiZOsdV49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df[df.Split == 'Train']\n",
        "val_df = df[df.Split == 'Val']\n",
        "test_df = df[df.Split == 'Test']"
      ],
      "metadata": {
        "id": "M17GYJ8XdYVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Todo:\n",
        "Common Feature Train\n",
        "All feature train"
      ],
      "metadata": {
        "id": "ekzs7ZOEa0I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_features = ['feature_05','feature_06','feature_07','feature_09','feature_10','feature_11','feature_12','feature_13','feature_14','feature_20','feature_22','feature_23','feature_24','feature_25','feature_28','feature_29','feature_30','feature_34','feature_35','feature_36','feature_38','feature_48','feature_49','feature_59','feature_60','feature_61','feature_67','feature_68','feature_69','feature_70','feature_71','feature_72']\n"
      ],
      "metadata": {
        "id": "dWk8UxLgaynV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling the Data"
      ],
      "metadata": {
        "id": "yfnpiT2pctce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df_feature_cols = []\n",
        "possible_feature_cols = [f'feature_{i:02}' for i in range(0,79)]\n",
        "\n",
        "for feature in df.columns:\n",
        "    if feature in possible_feature_cols:\n",
        "        df_feature_cols.append(feature)\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "df[df_feature_cols] = std_scaler.fit_transform(df[df_feature_cols])"
      ],
      "metadata": {
        "id": "3fP5LaltbkUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting the features"
      ],
      "metadata": {
        "id": "XgCYWFKvdngc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPORAL_FEATURES = ['date_id', 'time_id','symbol_id']\n",
        "MARKET_FEATURES = [f'feature_{i:02}' for i in range(0,79) if f'feature_{i:02}' in df.columns]\n",
        "RESPONDER_FEATURES = [f'responder_{i}' for i in range(0,9) if f'responder_{i}' in df.columns]\n",
        "RESPONDER_FEATURES.remove('responder_6')\n",
        "SYMBOL_FEATURES = [f'symbol_id_{i}' for i in range(max_symbol_id) if f'symbol_id_{i}' in df.columns]\n",
        "WEIGHT = ['WEIGHT']"
      ],
      "metadata": {
        "id": "ZSNYx9tldlQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_FEATURES = MARKET_FEATURES\n",
        "ALL_FEATURES += RESPONDER_FEATURES\n",
        "ALL_FEATURES = ALL_FEATURES + SYMBOL_FEATURES"
      ],
      "metadata": {
        "id": "q9PqWf9_dm7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_df[ALL_FEATURES]\n",
        "train_y = train_df[['responder_6']]\n",
        "\n",
        "val_x = val_df[ALL_FEATURES]\n",
        "val_y = val_df[['responder_6']]"
      ],
      "metadata": {
        "id": "qu_NWHH6drVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "vMLW5TDkbr-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "yq-YsAlFbtFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ridge Regression"
      ],
      "metadata": {
        "id": "EVRIss4GcykW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Ridge Regression\n",
        "# Typical values of alpha are 0.01 to 10,000\n",
        "\n",
        "ridge_mse = []\n",
        "\n",
        "# 50 alpha values from 0.01 to 10000 in logarithmic scaling\n",
        "alpha_counter = np.logspace(-2, 4, 50)\n",
        "# alpha_counter = np.arange(0.1,1000,0.1)\n",
        "\n",
        "for a in alpha_counter:\n",
        "    # Ridge Regression\n",
        "    ridge = Ridge(alpha=a)\n",
        "    ridge.fit(X_train, y_train)\n",
        "    ridge_y_pred = ridge.predict(X_test)\n",
        "    ridge_mse.append(mean_squared_error(y_test, ridge_y_pred))\n",
        "\n",
        "plt.plot(alpha_counter,ridge_mse)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "# argmin() finds index of minimnum value\n",
        "min_mse_index = np.argmin(ridge_mse)\n",
        "\n",
        "print(f'Minimum MSE = {ridge_mse[min_mse_index]} at alpha = {alpha_counter[min_mse_index]}')"
      ],
      "metadata": {
        "id": "i7Mf253Abv3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_mse = []\n",
        "Lalpha_counter = np.logspace(-3, 1, 50)\n",
        "\n",
        "for a in Lalpha_counter:\n",
        "    lasso = Lasso(alpha=a)\n",
        "    lasso.fit(X_train, y_train)\n",
        "    lasso_y_pred = lasso.predict(X_test)\n",
        "    lasso_mse.append(mean_squared_error(y_test, lasso_y_pred))\n",
        "\n",
        "plt.plot(Lalpha_counter,lasso_mse)\n",
        "plt.xscale('log')\n",
        "plt.show()\n",
        "\n",
        "# argmin() finds index of minimnum value\n",
        "min_mse_index = np.argmin(lasso_mse)\n",
        "\n",
        "print(f'Minimum MSE = {lasso_mse[min_mse_index]} at alpha = {Lalpha_counter[min_mse_index]}')"
      ],
      "metadata": {
        "id": "T4EZf9bEbx7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Elastic Results"
      ],
      "metadata": {
        "id": "W1TfGOGUc0zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_alpha_values = np.logspace(-3, 2, 10)  # 10 values from 0.001 to 100\n",
        "l1_ratios = np.linspace(0.1, 1.0, 10)  # 10 values from 0.1 to 1.0\n",
        "\n",
        "elastic_mse_results = np.zeros( (len(l1_alpha_values), len(l1_ratios))  )\n",
        "\n",
        "for i, a in enumerate(l1_alpha_values):\n",
        "    for j, l1 in enumerate(l1_ratios):\n",
        "        elastic_net = ElasticNet(alpha=a, l1_ratio=l1)\n",
        "        elastic_net.fit(X_train, y_train)\n",
        "        elastic_net_y_pred = elastic_net.predict(X_test)\n",
        "        elastic_mse_results[i, j] = mean_squared_error(y_test, elastic_net_y_pred)\n",
        "\n",
        "\n",
        "\n",
        "min_mse_index = np.unravel_index(np.argmin(elastic_mse_results, axis=None), elastic_mse_results.shape)\n",
        "optimal_alpha = l1_alpha_values[min_mse_index[0]]\n",
        "optimal_l1_ratio = l1_ratios[min_mse_index[1]]\n",
        "\n",
        "print(f\"Optimal Alpha: {optimal_alpha}\")\n",
        "print(f\"Optimal L1 Ratio: {optimal_l1_ratio}\")\n",
        "print(f\"Minimum MSE: {elastic_mse_results[min_mse_index]}\")"
      ],
      "metadata": {
        "id": "qI6zpGJFb0Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Trees and XGBoost"
      ],
      "metadata": {
        "id": "9WlbJAfadd6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
        "\n",
        "# Models for regression\n",
        "regressors = {\n",
        "    \"Decision Tree\": DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"XGBoost\": xgb.XGBRegressor(tree_method=\"hist\")\n",
        "}\n",
        "\n",
        "# Metrics for regression\n",
        "metrics = {\n",
        "    \"Mean Squared Error\": mean_squared_error,\n",
        "    \"Mean Absolute Error\": mean_absolute_error,\n",
        "    \"R^2 Score\": r2_score,\n",
        "    \"Explained Variance\": explained_variance_score\n",
        "}"
      ],
      "metadata": {
        "id": "QUCz1DwFdgQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "use_val = True\n",
        "results_train = {}\n",
        "results_val = {}\n",
        "\n",
        "for model in regressors:\n",
        "    print(f\"Model: {model}\")\n",
        "\n",
        "    # Start timing\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    regressors[model].fit(train_x, train_y)\n",
        "\n",
        "    # End timing\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    # Predict and calculate metrics\n",
        "    pred_y = regressors[model].predict(train_x)\n",
        "    if use_val:\n",
        "        pred_y_val = regressors[model].predict(val_x)\n",
        "\n",
        "    train_list = []\n",
        "    val_list = []\n",
        "    for metric in metrics:\n",
        "        score = metrics[metric](train_y, pred_y)\n",
        "        result_string = f\"{metric}: Train - {round(score, 4)}\"\n",
        "        if use_val:\n",
        "            score2 = metrics[metric](val_y, pred_y_val)\n",
        "            result_string+= f\" Val - {round(score2,4)}\"\n",
        "            val_list.append(score2)\n",
        "        print(result_string)\n",
        "        train_list.append(score)\n",
        "\n",
        "    # Save results\n",
        "    results_train[model] = train_list\n",
        "    if use_val:\n",
        "        results_val[model] = val_list\n",
        "\n",
        "    # Print elapsed time\n",
        "    print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "FrsAYrZYd_wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Neural Nets"
      ],
      "metadata": {
        "id": "3wZY1fMPc4d2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "_weEZGVDb3us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=X_train.shape[1:]\n",
        "print(input_shape)"
      ],
      "metadata": {
        "id": "8xKf-ryab5Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=X_train_scaled.shape[1:]),\n",
        "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation(\"relu\"),\n",
        "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation(\"relu\"),\n",
        "\n",
        "    # Single output neuron for regression\n",
        "    # We want to predict just Responder_6\n",
        "    tf.keras.layers.Dense(1, activation=None)\n",
        "])\n",
        "\n",
        "tf.keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "aWe9cA_Eb610"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressors['Neural Net'].compile(loss=\"mse\", optimizer=\"adam\",\n",
        "              metrics=[\"mse\", \"mae\"])\n",
        "history = regressors['Neural Net'].fit(train_x.values.astype('float32'), train_y.values.astype('float32'),\n",
        "                             batch_size = 1024,\n",
        "                             validation_data = (val_x.values.astype('float32'), val_y.values.astype('float32')) if use_val else None,\n",
        "                             epochs=30)"
      ],
      "metadata": {
        "id": "DlEqC8S3eFp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot(\n",
        "    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
        "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
        "plt.legend(loc=\"lower left\")  # extra code\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sAqa1eJjeHF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = test_df[ALL_FEATURES]\n",
        "test_y = test_df[['responder_6']]"
      ],
      "metadata": {
        "id": "zULYcPqueIwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_y_test = regressors[model].predict(test_x.values.astype('float32'), batch_size = 1024)"
      ],
      "metadata": {
        "id": "pZcV-MbEeKNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performance\n",
        "results_test = {}\n",
        "for model in regressors:\n",
        "  print(f\"Model: {model}\")\n",
        "  if 'Neural Net' in model:\n",
        "      pred_y_test = regressors[model].predict(test_x.values.astype('float32'),\n",
        "                                             batch_size = 1024)\n",
        "  else:\n",
        "      pred_y_test = regressors[model].predict(test_x)\n",
        "\n",
        "  results = []\n",
        "  for metric in metrics:\n",
        "    score = metrics[metric](test_y, pred_y_test)\n",
        "    print(f\"{metric}: {round(score,4)}\")\n",
        "    results.append(score)\n",
        "  results_test[model] = results\n",
        "\n",
        "  print()"
      ],
      "metadata": {
        "id": "BEqad9NOeLcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance"
      ],
      "metadata": {
        "id": "1MuLOKdfeNKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_importance\n",
        "plt.figure(figsize = (10,20))\n",
        "\n",
        "plot_importance(regressors[\"XGBoost\"])\n",
        "plt.savefig('feature_importance.png', dpi = 300)"
      ],
      "metadata": {
        "id": "d91GHMOleOqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nTeCkKx9ayLb"
      }
    }
  ]
}