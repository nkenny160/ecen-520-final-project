{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\ndf = pd.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")\ndf_pl = pl.read_parquet(\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\")\n\n# Three Cases\n# 1. Fully Empty columns : we can just drop them\n# 2. Partially Empty Columns: we can impute them\n# 3. Full Columns : No imputation necessary\n\nempty_columns = []\nfully_filled_columns = []\npartially_empty_columns = []\n\nfor feature in df_pl.columns:\n    # Count empty and non-empty rows\n    empty_rows = df_pl[feature].is_null().sum()\n    nonempty_rows = len(df_pl[feature]) - empty_rows\n\n    # Classify the columns based on the counts\n    if nonempty_rows == 0:\n        empty_columns.append(feature)\n    elif empty_rows == 0:\n        fully_filled_columns.append(feature)\n    else:\n        partially_empty_columns.append(feature)\n\n# We don't use df_pl again, so delete it to conserve memory\ndel df_pl\n\ndf.sort_values(['time_id','date_id'])\n\n# drop empty columns using imputer\ndf = df.drop(empty_columns, axis = 1)\n\n# Fill in partially empty columns\nfor feature in partially_empty_columns:\n    df[feature] = df.groupby('symbol_id')[feature].transform(lambda x: x.ffill().bfill())\n\n# Split data temporally - in partition0 there are 170 days and 849 unique time ids per day\ndf = df.sort_values(['date_id', 'time_id'])\ndate_counts = df.date_id.value_counts()\ndate_counts = pd.DataFrame(date_counts.sort_index())\ndate_counts['cumulative_sum'] = date_counts['count'].cumsum()\n\n# Organize columns into different groups\nTIME_FEATURES = ['relative_timestamp']\nMARKET_FEATURES = [f'feature_{i:02}' for i in range(0,79) if f'feature_{i:02}' in df.columns]\nRESPONDER_FEATURES = [f'responder_{i}' for i in range(0,9) if f'responder_{i}' in df.columns]\n# RESPONDER_FEATURES.remove('responder_6')\nSYMBOL_FEATURES = ['symbol_id']\n\n# Normalize Data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncols_to_normalize = MARKET_FEATURES + RESPONDER_FEATURES\ndf[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])\n\n# Determine splitting points\ntotal = len(df)\ntrain_percentage = 0.6\nval_percentage = 0.2\ntest_percentage = 0.2\napprx_train_len = int(total*train_percentage)\napprx_val_len = int(total*val_percentage) \napprx_test_len = total - apprx_train_len - apprx_val_len\n\n# Determine Splitting points\n\ndef split_func(row):\n    s = row['cumulative_sum']\n    if s <= apprx_train_len:\n        return 'Train'\n    elif (s > apprx_train_len) and (s <= apprx_train_len + apprx_val_len):\n        return 'Val'\n    elif (s > apprx_train_len + apprx_val_len):\n        return 'Test'\n    else:\n        raise ValueError\n\ndate_counts['Split'] = date_counts.apply(split_func, axis = 1)\n# print(date_counts.Split.value_counts())\n\nlast_train_data = date_counts[date_counts.Split == 'Train'].tail(1) \nfirst_test_data = date_counts[date_counts.Split == 'Test'].head(1)\n\n# Once Splitting points are determined, then make the necessary splits\n \ndef split_func_df(row):\n    s = row['date_id']\n    if s <= last_train_data.index[0]:\n        return 'Train'\n    elif (s > last_train_data.index[0]) and (s < first_test_data.index[0]):\n        return 'Val'\n    elif (s >= first_test_data.index[0]):\n        return 'Test'\n    else:\n        raise ValueError\n\n# Potentially encode time_id as sine/cosine to capture cyclical nature\n# train_df['time_sin'] = np.sin(2 * np.pi * train_df['time_id'] / max_time_id)\n# train_df['time_cos'] = np.cos(2 * np.pi * train_df['time_id'] / max_time_id)\n\ndf['Split'] = df.apply(split_func_df, axis = 1)\ndf['Split'].value_counts()\n\ntrain_df = df[df.Split == 'Train']\nval_df = df[df.Split == 'Val']\ntest_df = df[df.Split == 'Test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T06:38:36.026849Z","iopub.execute_input":"2024-12-01T06:38:36.027285Z","iopub.status.idle":"2024-12-01T06:39:09.107535Z","shell.execute_reply.started":"2024-12-01T06:38:36.027246Z","shell.execute_reply":"2024-12-01T06:39:09.106817Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# LSTM model testing","metadata":{}},{"cell_type":"code","source":"import numpy as np\ndef create_sequences(data, sequence_length):\n    X, y = [], []\n    for i in range(len(data) - sequence_length):\n        seq = data[i:i+sequence_length]\n        target = data['responder_6'].iloc[i+sequence_length]\n        X.append(seq)\n        y.append(target)\n    return np.array(X), np.array(y)\n\n# Create sequences\nfeatures = ['date_id', 'time_id', 'feature_06', 'feature_07', 'feature_05', 'feature_68', 'symbol_id', 'responder_6']\nX_train, y_train = create_sequences(train_df[features], sequence_length=50)\nX_val, y_val = create_sequences(val_df[features], sequence_length=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T06:39:22.509735Z","iopub.execute_input":"2024-12-01T06:39:22.510440Z","iopub.status.idle":"2024-12-01T06:42:43.792748Z","shell.execute_reply.started":"2024-12-01T06:39:22.510389Z","shell.execute_reply":"2024-12-01T06:42:43.792038Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import LSTM, Dense\n\n# model = Sequential()\n# model.add(LSTM(50, activation='relu', input_shape=(window_size, num_features)))\n# model.add(Dense(1))  # Predicting a single value\n# model.compile(optimizer='adam', loss='mse')\n# model.fit(X_train, y_train, epochs=50, batch_size=32)\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.mixed_precision import set_global_policy\nfrom tensorflow.keras.callbacks import TensorBoard\n\n# Enable mixed precision\nset_global_policy('mixed_float16')\n\n# Enable XLA optimization\ntf.config.optimizer.set_jit(True)\n\n# Define the model\nmodel = Sequential([\n    LSTM(64, activation='tanh', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n    Dropout(0.2),\n    LSTM(32, activation='tanh'),\n    Dropout(0.2),\n    Dense(1)  # Output layer for regression\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\n\n# Create TensorFlow datasets\nbatch_size = 128  # Adjust for optimal GPU utilization\ntrain_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\nval_data = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# TensorBoard callback with profiling\ntensorboard_cb = TensorBoard(log_dir=\"./logs\", profile_batch='2,10')\n\n# Train the model\nhistory = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=20,\n    verbose=1,\n    callbacks=[tensorboard_cb]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T06:43:02.572797Z","iopub.execute_input":"2024-12-01T06:43:02.573579Z","iopub.status.idle":"2024-12-01T07:06:25.387925Z","shell.execute_reply.started":"2024-12-01T06:43:02.573542Z","shell.execute_reply":"2024-12-01T07:06:25.387254Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1733035403.709660     120 service.cc:145] XLA service 0x5cbd87e92d40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1733035403.709703     120 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1733035403.709707     120 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1733035403.852975     123 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 8ms/step - loss: 0.9134 - mae: 0.6206 - val_loss: 1.0376 - val_mae: 0.6602\nEpoch 2/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 8ms/step - loss: 0.9069 - mae: 0.6163 - val_loss: 1.0376 - val_mae: 0.6602\nEpoch 3/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.9067 - mae: 0.6163 - val_loss: 1.0375 - val_mae: 0.6601\nEpoch 4/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.9063 - mae: 0.6162 - val_loss: 1.0375 - val_mae: 0.6602\nEpoch 5/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.9055 - mae: 0.6161 - val_loss: 1.0375 - val_mae: 0.6602\nEpoch 6/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.9021 - mae: 0.6166 - val_loss: 1.0348 - val_mae: 0.6601\nEpoch 7/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.8856 - mae: 0.6177 - val_loss: 1.0163 - val_mae: 0.6622\nEpoch 8/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.8667 - mae: 0.6134 - val_loss: 0.9709 - val_mae: 0.6437\nEpoch 9/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.8024 - mae: 0.5954 - val_loss: 0.9491 - val_mae: 0.6395\nEpoch 10/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7668 - mae: 0.5828 - val_loss: 0.9496 - val_mae: 0.6424\nEpoch 11/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7517 - mae: 0.5784 - val_loss: 0.9091 - val_mae: 0.6314\nEpoch 12/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7427 - mae: 0.5766 - val_loss: 0.9337 - val_mae: 0.6385\nEpoch 13/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7323 - mae: 0.5733 - val_loss: 1.1543 - val_mae: 0.7445\nEpoch 14/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7216 - mae: 0.5703 - val_loss: 0.9748 - val_mae: 0.6515\nEpoch 15/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7219 - mae: 0.5716 - val_loss: 0.8654 - val_mae: 0.6251\nEpoch 16/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.6951 - mae: 0.5612 - val_loss: 0.8561 - val_mae: 0.6224\nEpoch 17/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.6955 - mae: 0.5639 - val_loss: 0.8812 - val_mae: 0.6247\nEpoch 18/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7253 - mae: 0.5729 - val_loss: 0.8245 - val_mae: 0.6070\nEpoch 19/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7032 - mae: 0.5655 - val_loss: 0.8336 - val_mae: 0.6120\nEpoch 20/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.6852 - mae: 0.5610 - val_loss: 0.8684 - val_mae: 0.6156\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"history = model.fit(\n    train_data,\n    validation_data=val_data,\n    epochs=20,\n    verbose=1,\n    callbacks=[tensorboard_cb]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T07:11:08.628306Z","iopub.execute_input":"2024-12-01T07:11:08.628897Z","iopub.status.idle":"2024-12-01T07:33:55.717191Z","shell.execute_reply.started":"2024-12-01T07:11:08.628864Z","shell.execute_reply":"2024-12-01T07:33:55.716483Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7421 - mae: 0.5828 - val_loss: 0.8990 - val_mae: 0.6190\nEpoch 2/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7291 - mae: 0.5784 - val_loss: 0.8653 - val_mae: 0.6246\nEpoch 3/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.6961 - mae: 0.5634 - val_loss: 0.9520 - val_mae: 0.6333\nEpoch 4/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.6916 - mae: 0.5643 - val_loss: 0.9896 - val_mae: 0.6523\nEpoch 5/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7195 - mae: 0.5762 - val_loss: 0.9825 - val_mae: 0.6547\nEpoch 6/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.8013 - mae: 0.5975 - val_loss: 0.8977 - val_mae: 0.6291\nEpoch 7/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7167 - mae: 0.5731 - val_loss: 0.8293 - val_mae: 0.6126\nEpoch 8/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7434 - mae: 0.5824 - val_loss: 0.8714 - val_mae: 0.6242\nEpoch 9/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7344 - mae: 0.5787 - val_loss: 0.9226 - val_mae: 0.6641\nEpoch 10/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7495 - mae: 0.5849 - val_loss: 0.8449 - val_mae: 0.6089\nEpoch 11/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7185 - mae: 0.5742 - val_loss: 0.9810 - val_mae: 0.6566\nEpoch 12/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7487 - mae: 0.5852 - val_loss: 0.9469 - val_mae: 0.6460\nEpoch 13/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7516 - mae: 0.5855 - val_loss: 0.9129 - val_mae: 0.6257\nEpoch 14/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7183 - mae: 0.5738 - val_loss: 0.9561 - val_mae: 0.6640\nEpoch 15/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7281 - mae: 0.5764 - val_loss: 0.9616 - val_mae: 0.6476\nEpoch 16/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 8ms/step - loss: 0.7530 - mae: 0.5885 - val_loss: 0.8415 - val_mae: 0.6116\nEpoch 17/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.7767 - mae: 0.5933 - val_loss: 0.9570 - val_mae: 0.6438\nEpoch 18/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.8215 - mae: 0.6012 - val_loss: 1.0279 - val_mae: 0.6593\nEpoch 19/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.8874 - mae: 0.6163 - val_loss: 1.0304 - val_mae: 0.6617\nEpoch 20/20\n\u001b[1m9047/9047\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 8ms/step - loss: 0.8748 - mae: 0.6162 - val_loss: 1.0240 - val_mae: 0.6600\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get the data for responder_6 in the test set\nresponder_6_test_data = test_df[['date_id', 'time_id', 'feature_06', 'feature_07', 'feature_05', 'feature_68', 'symbol_id']]  # Assuming X_test is your test set features\nresponder_6_actual_values = test_df[responder_6]  # Assuming y_test is your test set labels\n\n# Make predictions for responder_6's test data\nresponder_6_predictions = model.predict(responder_6_test_data)\n\n# Plot the actual vs predicted values\nplt.figure(figsize=(10, 6))  # Adjust figure size as needed \nplt.scatter(range(len(responder_6_actual_values)), responder_6_actual_values, label='Actual Values')\nplt.plot(range(len(responder_6_predictions.squeeze())), responder_6_predictions.squeeze(), label='Predicted Values')  # Squeeze to remove extra dimension\nplt.xlabel('Time Step')\nplt.ylabel('Values')\nplt.title(f'Responder 6 - Actual vs Predicted Values')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}